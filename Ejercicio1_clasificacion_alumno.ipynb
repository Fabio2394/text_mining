{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fabio2394/text_mining/blob/main/Ejercicio1_clasificacion_alumno.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M5MhVlxlKxJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab5c1b8-be89-493b-be3e-7a8c370bdfba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dffd1b"
      },
      "source": [
        "\n",
        "‚öôÔ∏è **Requerimientos importantes sobre el ejercicio**\n",
        "\n",
        "- El notebook debe ejecutarse **de principio a fin sin intervenci√≥n manual**.\n",
        "- Si utilizas librer√≠as que no est√°n incluidas por defecto en Google Colab, **aseg√∫rate de instalarlas dentro del notebook** (por ejemplo: `!pip install ...`).\n",
        "\n",
        "- Algunas celdas incluyen identificadores especiales que indican ciertas normas que **debes** respetar:\n",
        " - `#NO-MODIFY: DATA LOAD`  \n",
        "    üîí **No modifiques** el contenido de esta celda.\n",
        "\n",
        "  - `#NO-MODIFY: VARIABLE NAME`  \n",
        "    ‚úèÔ∏è Puedes modificar o a√±adir informaci√≥n **dentro de la celda**, pero **sin cambiar el nombre de la variable asignada**. No incluyas m√°s variables de las existentes en la celda.\n",
        "\n",
        "  - `#MODIFY: ADD INFO TO SOLVE FUNCTION`  \n",
        "    üîß Puedes modificar el **interior de la funci√≥n** para resolver la tarea, pero **no cambies su nombre, la cabecera ni el `return`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w1C3ZBXHL0b"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WTXfHYTHQmo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5xkvK9cHSse"
      },
      "outputs": [],
      "source": [
        "# Add your imports here\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1KKTZUcH_xk"
      },
      "source": [
        "# üîç Ejercicio1: Detecci√≥n de profesiones en tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enunciado"
      ],
      "metadata": {
        "id": "gLDLR2bEHk2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio vamos a trabajar con un conjunto de datos procedente de medios sociales online.\n",
        "\n",
        "Utilizaremos un subconjunto de los datos de la tarea 1 del shared task [**ProfNER**](https://temu.bsc.es/smm4h-spanish), centrada en la detecci√≥n de menciones a profesiones en tweets publicados durante la pandemia del COVID-19. El objetivo original de la tarea era analizar que profesiones podr√≠an haber sido especialmente vulnerables en el contexto de la crisis sanitaria.\n",
        "\n",
        "Para simplificar el ejercicio, he preparado una versi√≥n reducida del dataset original. Tu tarea ser√° entrenar un clasificador binario basado en la arquitectura Transformers, que, dado un tweet, determine si contiene una menci√≥n expl√≠cita a una profesi√≥n (etiqueta `1`) o no (etiqueta `0`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kx1_gwPQGcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Objetivos del ejercicio**\n",
        "\n",
        "A lo largo de este notebook, completar√°s las siguientes etapas para construir un clasificador de menciones a profesiones en tweets:\n",
        "\n",
        "1. **An√°lisis Exploratorio de Datos (EDA)**: Calcular estad√≠sticas b√°sicas del conjunto de datos (como el n√∫mero de ejemplos del training set, la distribuci√≥n de clases del dataset, la longitud media de los textos) o crear visualizaciones para cmprender mejor el contenido de los documentos usando wordclouds o histogramas.\n",
        "\n",
        "2. **Selecci√≥n y justificaci√≥n del modelo**: Elegir un modelo del Hub de Huggingface adecuado para los datos con los que se va a trabajar y el tipo de tarea a desarrollar.\n",
        "\n",
        "3. **Entrenamiento del clasificador**: Entrenar el modelo de forma reproducible y evaluar su rendimiento sobreel conjunto de datos de validaci√≥n, incluyendo un classification score y matriz de confusion\n",
        "\n",
        "4. **Generaci√≥n de predicciones sobre el conjunte de test**: Aplicar el modelo entrenado al conjunto de test, y guardar las predicciones en un archivo `.tsv` de 2 columnas `id` y `label` separadas por tabulador"
      ],
      "metadata": {
        "id": "3VHMEt0x-7UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù **Criterios de Evaluaci√≥n**\n",
        "\n",
        "Tu trabajo ser√° evaluado seg√∫n los siguientes criterios:\n",
        "\n",
        "| Criterio                                            | Peso  |\n",
        "|-----------------------------------------------------|--------|\n",
        "| üîç An√°lisis exploratorio y preprocesamiento         | 20%   |\n",
        "| ü§ñ Selecci√≥n y justificaci√≥n del modelo             | 25%   |\n",
        "| üìÅ Formato y validez del archivo de predicciones    | 5%    |\n",
        "| ‚öôÔ∏è Ejecuci√≥n correcta del notebook (sin intervenci√≥n) | 10%   |\n",
        "| üìà Rendimiento del modelo sobre el conjunto de test | 30%   |\n",
        "| ‚úçÔ∏è Claridad y calidad de las explicaciones          | 10%   |\n",
        "\n",
        "\n",
        "\n",
        "üîî **Nota importante:**\n",
        "\n",
        "> El rendimiento del modelo se evaluar√° utilizando m√©tricas est√°ndar como el **F1-score** sobre el conjunto de test.\n",
        "\n",
        "> El archivo de predicciones debe respetar **estrictamente** el formato solicitado (`id` y `label`, separados por tabulador y con extensi√≥n `.tsv`).  \n",
        "  ‚ùó Si el archivo no cumple con este formato, **el ejercicio no podr√° ser evaluado en esa secci√≥n**.\n",
        "\n",
        "> El/la estudiante con el **mayor F1-score** obtendr√° la puntuaci√≥n m√°xima en el apartado de rendimiento. El resto de calificaciones se ajustar√°n de forma proporcional al mejor resultado\n"
      ],
      "metadata": {
        "id": "VPaXLRNeAElo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚öôÔ∏è **Requerimientos y reglas**\n",
        "\n",
        "- El notebook debe ejecutarse **de principio a fin sin intervenci√≥n manual**.\n",
        "- Si utilizas librer√≠as que no est√°n incluidas por defecto en Google Colab, **aseg√∫rate de instalarlas dentro del notebook** (por ejemplo: `!pip install ...`).\n",
        "\n",
        "- Algunas celdas incluyen identificadores especiales que indican ciertas normas que **debes** respetar:\n",
        " - `#NO-MODIFY: DATA LOAD`  \n",
        "    üîí **No modifiques** el contenido de esta celda.\n",
        "\n",
        "  - `#NO-MODIFY: VARIABLE NAME`  \n",
        "    ‚úèÔ∏è Puedes modificar o a√±adir informaci√≥n **dentro de la celda**, pero **sin cambiar el nombre de la variable asignada**. No incluyas m√°s variables de las existentes en la celda.\n",
        "\n",
        "  - `#MODIFY: ADD INFO TO SOLVE FUNCTION`  \n",
        "    üîß Puedes modificar el **interior de la funci√≥n** para resolver la tarea, pero **no cambies su nombre, la cabecera ni el `return`**.\n"
      ],
      "metadata": {
        "id": "3M19bykxA0ZP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj9IcJEwIB91"
      },
      "source": [
        "# Tu resoluci√≥n (rellena las celdas marcadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOXm4JVwIElL"
      },
      "source": [
        "## Obtenci√≥n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos los datos del [repositorio de Huggingface](https://huggingface.co/datasets/luisgasco/profner_classification_master)."
      ],
      "metadata": {
        "id": "gtOwX5HKCSfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aiOIGd78IG3y"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
        "dataset = load_dataset(\"luisgasco/profner_classification_master\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset contiene tres subsets:\n",
        "- **train** y **validation**: Contienen el identificador del tweet, el texto, y su etiqueta, que podr√° tener valor 1, si contiene una menci√≥n de una profesi√≥n; o valor 0, si no contiene una menci√≥n de una profesi√≥n.\n",
        "- **test**: El test set tambi√≠en contiene la informaci√≥n de label por un requerimiento de Huggingface, pero el contenido de esta variable es siempre \"-1\". Es decir que deber√©is predecir nuevas etiquetas una vez hay√°is entrenado el modelo utilizando el train y el validation set."
      ],
      "metadata": {
        "id": "nY-vjg88CfpW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163nmdUnIG-P"
      },
      "source": [
        "## An√°lisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para hacer el an√°lisis exploratorio de datos, transformamos cada subset a un pandas dataframe para mayor comodidad."
      ],
      "metadata": {
        "id": "umz-kP7yDDkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkphOXpCIhqj"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "dataset_train_df = dataset[\"train\"].to_pandas()\n",
        "dataset_val_df = dataset[\"validation\"].to_pandas()\n",
        "dataset_test_df = dataset[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df.head()"
      ],
      "metadata": {
        "id": "epAHtyj4YXaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√∫mero de documentos**\n",
        "\n",
        "Obten con la funci√≥n `get_num_docs_evaluation()` el n√∫mero de documentos del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ],
      "metadata": {
        "id": "kJ4lLzjODJTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def get_num_docs_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  for num_docs in dataset_df:\n",
        "    num_docs = len(dataset_df)\n",
        "  # No modifiques el return\n",
        "  return num_docs\n"
      ],
      "metadata": {
        "id": "8v9AXqgMEh0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "PVxQg5u_FPlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la funci√≥n\n",
        "print(f'El dataset train tiene {get_num_docs_evaluation(dataset_train_df)} documentos')\n",
        "print(f'El dataset validation tiene {get_num_docs_evaluation(dataset_val_df)} documentos')\n"
      ],
      "metadata": {
        "id": "5l82IvmOFUBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√∫mero de documentos duplicados**\n",
        "\n",
        "Obten con la funci√≥n `detect_duplicates_evaluation()` el n√∫mero de documentos duplicados del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ],
      "metadata": {
        "id": "TrxV3XkcFYhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_cviBv-IxHJ"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def detect_duplicates_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  for num_duplicates in dataset_df:\n",
        "    num_duplicates = np.sum(dataset_df.duplicated())\n",
        "  # No modifiques el return\n",
        "  return num_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "y_4zYMnOFjhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la funci√≥n\n",
        "print(f'El dataset train tiene {detect_duplicates_evaluation(dataset_train_df)} documentos duplicados')\n",
        "print(f'El dataset validation tiene {detect_duplicates_evaluation(dataset_val_df)} documentos duplicados')"
      ],
      "metadata": {
        "id": "4AgUKaVQFjhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZq8ZXzkJk0t"
      },
      "source": [
        "**N√∫mero de documentos por cada clase:**\n",
        "\n",
        "\n",
        "Obten con la funci√≥n `analyse_num_labels_evaluation()` para calcular el n√∫mero de documentos de cada categor√≠a en el dataset\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvfd9yVVJo6-"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def analyse_num_labels_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  num_positives = dataset_df[dataset_df['label'] == 1].shape[0]\n",
        "  num_negatives = dataset_df[dataset_df['label'] == 0].shape[0]\n",
        "  # No modifiques el return\n",
        "  return num_positives, num_negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "zuRURsOqGVF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la funci√≥n\n",
        "num_pos_train, num_neg_train = analyse_num_labels_evaluation(dataset_train_df)\n",
        "num_pos_val, num_neg_val = analyse_num_labels_evaluation(dataset_val_df)\n",
        "\n",
        "print(f'El dataset train tiene {num_pos_train} documentos positivos y {num_neg_train} negativos')\n",
        "print(f'El dataset validation tiene {num_pos_val} documentos positivos y {num_neg_val} negativos')\n"
      ],
      "metadata": {
        "id": "IUx_-_wFGVGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax, fig = plt.subplots()\n",
        "etiquetas = dataset_train_df.label.value_counts()\n",
        "etiquetas.plot(kind= 'bar', color= [\"blue\", \"green\"])\n",
        "plt.title('Bar chart')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F-D3GzoYzs96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset de train tiene un buen balanceo de datos como podemos ver en el grafico (1393 documentos en ambas partes)."
      ],
      "metadata": {
        "id": "x8Q73RYI2L3e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxXknuNvKK8f"
      },
      "source": [
        "**Distribuci√≥n de la longitud de los tweet en caracteres:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_rLDRgCKU8j"
      },
      "outputs": [],
      "source": [
        "dataset_train_df[\"char_len\"] = dataset_train_df[\"text\"].apply(lambda x: len(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df.head()"
      ],
      "metadata": {
        "id": "L4rUQzPJr6VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.displot(data=dataset_train_df, x=\"char_len\", hue=\"label\", kde=True, legend=True)\n",
        "plt.legend([\"Profesion\", \"No Profesion\"])\n",
        "\n",
        "plt.title('Distribucion longitud caracteres', fontsize=16)\n",
        "plt.xlabel('Caracteres', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mqg5YYtNxYj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los tweets con Profesion son los mas largos, probablemente estan contando como les ha afectado a su trabajo la pandemia."
      ],
      "metadata": {
        "id": "I5HOhdAI5Dsv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrtI16QKKNZq"
      },
      "source": [
        "**An√°lisis de contenido de los tweets**\n",
        "\n",
        "Para ello utiliza wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZCi84lYK4jW"
      },
      "outputs": [],
      "source": [
        "txt_cat0 = \",\".join(dataset_train_df[dataset_train_df.label==0].text.to_list())\n",
        "txt_cat1 = \",\".join(dataset_train_df[dataset_train_df.label==1].text.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_stopwords = stopwords.words('spanish')\n",
        "\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=0,\n",
        "                      contour_color='steelblue', stopwords=es_stopwords,\n",
        "                      normalize_plurals = True)"
      ],
      "metadata": {
        "id": "Eot6NNkQt5jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label 0\n",
        "wordcloud.generate(txt_cat0)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "8DGgGeveua5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label 1\n",
        "wordcloud.generate(txt_cat1)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "UkKRR6_UukVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En ambas categorias hay muchas menciones a sitios web, al coronavirus y a muchas cosas relacionadas con ello, como pandemia, confinamiento, etc.\n",
        "Si que en la categoria 1 podemos visualizar varias palabras como erte, trabajo y trabajadores, que aunque no hay menciones a trabajos especificos, nos puede indicar que en los tweets se este hablando de profesiones."
      ],
      "metadata": {
        "id": "Iy8DxJ9F8gxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar espacios\n",
        "def eliminar_espacios(text):\n",
        "    return  \" \".join(text.split())\n",
        "\n",
        "# Minusculas\n",
        "def texto_to_lower(text):\n",
        "  return text.lower()"
      ],
      "metadata": {
        "id": "uQvPiKcrgLmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Espacios y minusculas\n",
        "tqdm.pandas()\n",
        "\n",
        "dataset_train_df[\"normalized\"] = dataset_train_df[\"text\"].progress_apply(lambda x: eliminar_espacios(x))\n",
        "dataset_train_df[\"normalized\"] = dataset_train_df[\"normalized\"].progress_apply(lambda x: texto_to_lower(x))\n",
        "\n",
        "dataset_train_df.head()"
      ],
      "metadata": {
        "id": "RCSUgbSrgSTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizador de tweets\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "def normalizar_tokens(doc):\n",
        "    # Dividir el documento en palabras\n",
        "    palabras =  tweet_tokenizer.tokenize(doc)\n",
        "\n",
        "    # Reemplazar URLs, menciones de Twitter y n√∫meros por los tokens correspondientes\n",
        "    for i in range(len(palabras)):\n",
        "        if palabras[i].startswith(\"http://\") or palabras[i].startswith(\"https://\") or palabras[i].startswith(\"www.\"):\n",
        "            palabras[i] = \"URL\"\n",
        "        elif palabras[i].startswith(\"@\"):\n",
        "            palabras[i] = \"MENTION\"\n",
        "        elif palabras[i].isdigit():\n",
        "            palabras[i] = \"NUM\"\n",
        "\n",
        "    # Unir las palabras de nuevo en un documento modificado\n",
        "    doc_1 = ' '.join(palabras)\n",
        "\n",
        "    return doc_1"
      ],
      "metadata": {
        "id": "RQVwQ0MHjeii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df[\"normalized\"] = dataset_train_df[\"normalized\"].progress_apply(lambda x: normalizar_tokens(x))"
      ],
      "metadata": {
        "id": "jAN1wWEAk-28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df.normalized.to_list()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PKtInX4Dlhlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se est√° utilizando un modelo peque√±o de procesamiento de lenguaje natural para espa√±ol para un proceso de lematizacion\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def lematizar_eliminacion_tokens(texto):\n",
        "    # Procesar el texto con el objeto nlp\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Lematizar el texto\n",
        "    lemas = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Eliminar s√≠mbolos de puntuaci√≥n y stopwords\n",
        "    tokens_filtrados = [token for token in lemas if token.isalpha() and token.lower() not in es_stopwords]\n",
        "\n",
        "    # Unir los tokens filtrados en un nuevo texto\n",
        "    texto_procesado = ' '.join(tokens_filtrados)\n",
        "\n",
        "    return texto_procesado"
      ],
      "metadata": {
        "id": "ZGMw42iilpuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df[\"preprocessed_text\"] = dataset_train_df[\"normalized\"].progress_apply(lambda x: lematizar_eliminacion_tokens(x))"
      ],
      "metadata": {
        "id": "CmHSbk62l7Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_cat0 = \",\".join(dataset_train_df[dataset_train_df.label==0].preprocessed_text.to_list())\n",
        "txt_cat1 = \",\".join(dataset_train_df[dataset_train_df.label==1].preprocessed_text.to_list())\n",
        "\n",
        "# Label 0\n",
        "wordcloud.generate(txt_cat0)\n",
        "\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "RXToHLASnyUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label 1\n",
        "wordcloud.generate(txt_cat1)\n",
        "\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "rLBdw28loCWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZUV6mSIJO1"
      },
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El texto del dataset no est√° preparado para ser introducido en un modelo Transformers. Lleva a cabo el proceso de tokenizaci√≥n."
      ],
      "metadata": {
        "id": "61YNAqIAG3aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGA4Rz7UIDUz"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecciona un modelo apropiado para la tarea:"
      ],
      "metadata": {
        "id": "kGZStD5MHBuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Recuerda que en la siguiente celda s√≥lo debes asignar un valor a model_name. No a√±adas m√°s informaci√≥n en la celda."
      ],
      "metadata": {
        "id": "famSsepTHJPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He decidido escoger este modelo porque estoy trabajando con tweets en espa√±ol y en especifico el modelo ha sido entrenado con datos laborales, que se acerca mucho al objetivo de la tarea.\n",
        "\n",
        "He decidido escoger este modelo porque es una versi√≥n de BERT entrenada espec√≠ficamente para espa√±ol, y es una de las m√°s recomendadas para tareas de NLP en ese idioma"
      ],
      "metadata": {
        "id": "_enNxF6gY7Or"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svQiqzz_Lywz"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "model_name = 'dccuchile/bert-base-spanish-wwm-cased'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes continuar con el proceso aqu√≠:"
      ],
      "metadata": {
        "id": "lMivggEnHQqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(dataset_train_df)\n",
        "val_dataset = Dataset.from_pandas(dataset_val_df)\n",
        "test_dataset = Dataset.from_pandas(dataset_test_df)\n",
        "\n",
        "# Tokeniza\n",
        "def preprocess_function(examples):\n",
        "    # Tokenizar el texto\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "v6CH_9QpW6IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "7rZrTVJXWFXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUel8a-FN0nB"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga el model para ser ajustado posteriormente:\n",
        "\n"
      ],
      "metadata": {
        "id": "cOb2YTABXDv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"NO_Profesion\", 1: \"Profesion\"}\n",
        "label2id = {\"NO_Profesion\": 0, \"Profesion\": 1}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)"
      ],
      "metadata": {
        "id": "IzfR_vm5XHOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeUuTSJpN9pN"
      },
      "source": [
        "### Configuracion training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura los par√°metros de entrenamiento del modelo.\n",
        "\n",
        "\n",
        ">"
      ],
      "metadata": {
        "id": "0Iy9mSLIIkju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Recuerda que en la siguiente celda s√≥lo debes asignar atributos a la variable training_args. No a√±adas  otras variables en la celda"
      ],
      "metadata": {
        "id": "ucWL18iUIqme"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mexUYxXoN9v7"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"modelo_test\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.1,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    seed=52\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LosxnaBOIto"
      },
      "source": [
        "### M√©tricas de evaluaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define las m√©tricas de evaluaci√≥n"
      ],
      "metadata": {
        "id": "Xkg3xIBBIzdT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99-S1UCeOLAH"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1_score = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_value = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1_score_value = f1_score.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_value,\n",
        "        \"f1_score\": f1_score_value,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YFV7ugfOMtr"
      },
      "source": [
        "### Ajuste del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lleva a cabo el ajuste del modelo:"
      ],
      "metadata": {
        "id": "62fSC9hEI3ih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OYXvp-KOQ2J"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "YO5JONVzY1QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraemos los logs de entrenamiento\n",
        "history = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "# A veces los logs pueden tener pasos donde no se eval√∫a. Filtramos filas con 'loss' y 'eval_loss'\n",
        "train_loss = history[history['loss'].notna()][['step', 'loss']]\n",
        "eval_loss = history[history['eval_loss'].notna()][['step', 'eval_loss']]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_loss['step'], train_loss['loss'], label='Training Loss')\n",
        "plt.plot(eval_loss['step'], eval_loss['eval_loss'], label='Evaluation Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Evaluation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zWFQfx4lY83b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hi38oUIOevr"
      },
      "source": [
        "## Evaluacion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez llevada a cabo el entrenamiento, realiza la evaluaci√≥n del modelo."
      ],
      "metadata": {
        "id": "OPYzfaRRI-Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "xKzxF1nvozVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.remove_columns('label')"
      ],
      "metadata": {
        "id": "i_zsDM4cj0Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "rEOk5u3GXOhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genera predicciones"
      ],
      "metadata": {
        "id": "Sf1-X0ozH1x1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genera predicciones sobre el test set. Recuerda que el archivo que generes y adjuntes al ejercicio debe tener dos columnas:\n",
        "\n",
        "\n",
        "| id         | label |\n",
        "|------------|-------|\n",
        "| 1234567890 | 1     |\n",
        "| 1234567891 | 0     |\n",
        "| 1234567892 | 0     |\n",
        "| 1234567893 | 1     |\n",
        "\n",
        "- El archivo debe estar en formato **TSV** (separado por tabuladores).\n",
        "- Debe contener exactamente **dos columnas**: `id` y `label`.\n",
        "- Es obligatorio incluir la **cabecera**.\n"
      ],
      "metadata": {
        "id": "4GyrGKTfJUIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset)"
      ],
      "metadata": {
        "id": "Qf7sQSH_XQ20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predictions.predictions.argmax(axis=1)\n",
        "ids = [x[\"tweet_id\"] for x in test_dataset]"
      ],
      "metadata": {
        "id": "7fco-3GbbbKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    \"id\": ids,\n",
        "    \"label\": y_pred\n",
        "})\n",
        "\n",
        "df.to_csv(\"Savini_Fabio_ejercicio1_predicciones.tsv\", sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "64UwhSxdqivH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"Savini_Fabio_ejercicio1_predicciones.tsv\")"
      ],
      "metadata": {
        "id": "lrKvHyk4rFRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}